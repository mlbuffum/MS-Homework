---
title: "STAT 562 Homework 4"
author: "Maggie Buffum"
date: "February 7, 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem 5.35
Stirling's Formula (derived in Exercise 1.28), which gives an approximation for factorials, can be easily derived using the CLT.
  
(a) Argue that, if $X_i \sim$ exponential$(1),i=1,2,\dots,$ all independent, then for every $x$,
\[
P\bigg(\frac{\bar X_n - 1}{1/\sqrt{n}} \le x\bigg) \rightarrow P(Z \le x),
\]
  
where $Z$ is a standard normal random variable.
  
---
  
Let's first recall Stirling's Formula:
\[
n! \approx \sqrt{2\pi}n^{n+(1/2)}e^{-n}
\]
  
The *Central Limit Theorem* states that if we have a sequence of iid random variables $X_1,X_2,\dots$ with $EX_i = \mu$ and $VarX_i = \sigma^2 < \infty$, and define $\bar X_n = (1/n)\sum_{i=1}^n X_i$. Then,
\[
lim_{n\rightarrow \infty} \sqrt{n}(\bar X_n - \mu)/\sigma = \int_{-\infty}^x \frac{1}{\sqrt{2\pi}}e^{-y^2/2}dy
\]
  
Suppose we have $X_i \sim$ exponential$(1),i=1,2,\dots$, all independent. We know that $\bar X_n = \frac{1}{n}\sum_{i=1}^n X_i$, and the cdf of the exponential distribution with parameter 1 is $1 - e^{-x}$.
  
(b) Show that differentiating both sides of the approximation in part (a) suggests
\[
\frac{\sqrt{n}}{\Gamma(n)}\big(x\sqrt{n} + n\big)^{n-1}e^{-(x\sqrt{n} + n)} \approx \frac{1}{\sqrt{2\pi}}e^{-x^2/2},
\]
  
and that $x=0$ gives Stirling's Formula.
  
---
  
## Problem 5.39
This exercise, and the two following, will look at some of the mathematical details of convergence.
  
(a) Prove Theorem 5.5.4. (*Hint*: since $h$ is continuous, given $\epsilon > 0$ we can find a $\delta$ such that $|h(x_n) - h(x)| < \epsilon$ whenever $|x_n - x|<\delta$. Translate this into probability statements.)
  
---
  
Theorem 5.5.4 states that if $X_1,X_2,\dots$ converges in probability to a random variable $X$, then if $h$ is a continuous function, $h(X_1),h(X_2),\dots$ converges in probability to $h(X)$.
  
Let's first suppose that $X_1,X_2,\dots$ converge in probability to a random variable $X$. This means that
\[
lim_{n\rightarrow \infty}P(|X_n - X| < \delta) = 1.
\]
  
Consider a continuous function $h$. We want to show that
\[
lim_{n\rightarrow \infty}P(|h(x_n) - h(x)| < \epsilon) = 1.
\]
  
Since $h$ is continuous, we know by theorem that given $\epsilon > 0$ we can find $\delta$ such that $|h(x_n) - h(x)| < \epsilon$ whenever $|x_n - x| < \delta$. That is, $P(|h(x_n) - h(x)| < \epsilon) = 1$ given we have that $|x_n - x| < \delta$, so $P(|h(x_n) - h(x)| < \epsilon)$ comes down knowing under what conditions $|x_n - x| < \delta$. But we've already established that $|x_n - x| < \delta$ is always true as $n \rightarrow \infty$, which implies that $lim_{n\rightarrow \infty}P(|h(x_n) - h(x)| < \epsilon) = 1$ under the starting assumptions.
  
(b) In Example 5.5.8, find a subsequence of the $X_is$ that converges almost surely, that is, converges pointwise.
  
---
  
A sequence of random variables $X_1,X_2,\dots$ *converges almost surely* to a random variable $X$ is, for every $\epsilon > 0$,
\[
P(lim_{n \rightarrow \infty}|X_n - X|<\epsilon) = 1.
\]
  
In Example 5.5.8, the sample space $S$ is the closed interval [0,1], and $X_1,\dots,X_6$ are defined as
\[
\begin{aligned}
&X_1(s) = s + I_{[0,1]}(s)& X_2(s) = s + I_{[0,\frac{1}{2}]}(s) && X_3(s) = s + I_{[\frac{1}{2},1]}(s)\\
&X_4(s) = s + I_{[0,\frac{1}{3}]}(s)& X_5(s) = s + I_{[\frac{1}{3},\frac{2}{3}]}(s) && X_6(s) = s + I_{[\frac{2}{3},1]}(s)
\end{aligned}
\]
  
As the example shows, the sequence of all $X_is$ converges in probability but not pointwise. However, consider the following subsequence of $X_is$:
\[
\begin{aligned}
& X_7(s) = s + I_{[\frac{5}{4},\frac{3}{2}]}(s) & X_8(s) = s + I_{[\frac{3}{2},2]}(s)
\end{aligned}
\]
  
The indicator function will always be zero because no $s \in S$ satisfies the range specified for the indicator functions to be one. Therefore, $X_7,X_8$ converge pointwise to $X$.
  
## Problem 5.41
Prove Theorem 5.5.13; that is, show that
\[
P(|X_n - \mu| > \epsilon) \rightarrow 0 \text{ for every }\epsilon \iff P(X_n \le x) \rightarrow
\begin{cases}
0 &\text{if }x<\mu\\
1 &\text{if }x \ge \mu.
\end{cases}
\]
  
(a) Set $\epsilon = |x-\mu|$ and show that if $x > \mu$, then $P(X_n \le x) \ge P(|X_n - \mu| \le \epsilon)$, while if $x < \mu$, then $P(X_n \le x) \le P(|X_n-\mu| > \epsilon)$. Deduce the $\implies$ implication.
  
---
  
We will start by showing that if $P(|X_n - \mu| \le \epsilon)\rightarrow 0$, then $P(X_n \le x) \rightarrow 1$ for $x \ge \mu$ and $P(X_n \le x) \rightarrow 0$ for $x < \mu$. Let $\epsilon = |x-\mu|$.
  
Consider the case where $x > \mu$.
\[
\begin{aligned}
P(|X_n - \mu| \le \epsilon) &= P(|X_n - \mu| \le |x-\mu|)\\
&= P(|X_n - \mu| \le x-\mu)\text{ since }x>\mu\\
&= P(-(x-\mu) \le X_n - \mu \le x-\mu)\\
&= P(X_n - \mu \le x - \mu) - P(X_n - \mu \le -(x-\mu))\\
&\le P(X_n - \mu \le x - \mu)\\
&= P(X_n \le x)
\end{aligned}
\]
  
which gives us that
\[
P(X_n \le x) \ge P(|X_n - \mu| \le \epsilon)
\]
  
We know that $P(|X_n - \mu| > \epsilon) \rightarrow 0$ as $n \rightarrow \infty$, implying that $P(|X_n - \mu| \le \epsilon) \rightarrow 1$ as $n \rightarrow \infty$. Therefore, $P(X_n \le x) \rightarrow 1$ for $x > \mu$.
  
Consider the case where $x < \mu$.
\[
\begin{aligned}
P(|X_n - \mu| > \epsilon) &= P(|X_n - \mu| > |x-\mu|)\\
&= P(|X_n - \mu| > -(x-\mu))\\
&=P(X_n - \mu > -(x-\mu)) + P(X_n - \mu < x-\mu)\\
&\ge P(X_n - \mu < x-\mu)\\
&= P(X_n < x)
\end{aligned}
\]
  
such that
\[
P(X_n \le x) \le P(|X_n - \mu|>\epsilon)
\]
  
We began with the assumption that $P(|X_n - \mu| > \epsilon) \rightarrow 0$ when $n \rightarrow \infty$, and since $P(X_n \le x)$ cannot be less than 0, $P(X_n \le x) \rightarrow 0$ as $n \rightarrow \infty$.
  
(b) Use the fact that $\{x:|x-\mu| > \epsilon\} = \{x:x-\mu < -\epsilon\}\cup\{x:x-\mu > \epsilon\}$ to deduce the $\impliedby$ implication.
  
(See Billingsley 1995, Section 25, for a detailed treatment of the above results.)
  
---
  
Now we need to show that if $P(X_n \le x) \rightarrow 0$ when $x<\mu$ and $P(X_n \le x) \rightarrow 1$ when $x > \mu$ then $P(|X_n - \mu| > \epsilon) \rightarrow 0$.
  
Consider $\{x:|x-\mu| > \epsilon\}$






























