---
title: "STAT 562 Homework 4"
author: "Maggie Buffum"
date: "February 7, 2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem 5.35
Stirling's Formula (derived in Exercise 1.28), which gives an approximation for factorials, can be easily derived using the CLT.
  
(a) Argue that, if $X_i \sim$ exponential$(1),i=1,2,\dots,$ all independent, then for every $x$,
\[
P\bigg(\frac{\bar X_n - 1}{1/\sqrt{n}} \le x\bigg) \rightarrow P(Z \le x),
\]
  
where $Z$ is a standard normal random variable.
  
---
  
Suppose we have $X_i \sim$ exponential$(1),i=1,2,\dots$, all independent. We know that $\mu = 1$ and $\sigma^2 = 1$.
  
The *Central Limit Theorem* tells that
\[
\frac{(\bar X_n - \mu)}{\sigma/\sqrt{n}} = \frac{(\bar X_n - 1)}{1/\sqrt{n}}
\]
  
has (approximately) a standard normal distribution. Therefore, it directly follows that
\[
P\bigg(\frac{\bar X_n - 1}{1/\sqrt{n}} \le x\bigg) \rightarrow P(Z \le x)
\]
  
where $Z \sim n(0,1)$.
  
(b) Show that differentiating both sides of the approximation in part (a) suggests
\[
\frac{\sqrt{n}}{\Gamma(n)}\big(x\sqrt{n} + n\big)^{n-1}e^{-(x\sqrt{n} + n)} \approx \frac{1}{\sqrt{2\pi}}e^{-x^2/2},
\]
  
and that $x=0$ gives Stirling's Formula.
  
---
  
Let's start with the right side of of the equation:
\[
\frac{d}{dx}P(Z \le x) = \frac{d}{dx}F_Z(x) = f_Z(x) = \frac{1}{\sqrt{2\pi}}e^{-x^2/2}
\]
  
Now the left side of the equation,
\[
\begin{aligned}
\frac{d}{dx}P\bigg(\frac{\bar X_n - 1}{1/\sqrt{n}} \le x\bigg) &= \frac{d}{dx}\bigg(\frac{\big(\frac{1}{n}\sum_{i=1}^n X_i\big) - 1}{1/\sqrt{n}} \le x\bigg)\\
&= \frac{d}{dx}\bigg(\frac{\sqrt{n}}{n}\sum_{i=1}^n X_i \le x + \sqrt{n}\bigg)\\
&= \frac{d}{dx}\bigg(\sum_{i=1}^n X_i \le \big(x + \sqrt{n}\big)\frac{n}{\sqrt{n}}\bigg)\\
&= \frac{d}{dx}\bigg(\sum_{i=1}^n X_i \le x\sqrt{n} + n\bigg)\\
&\text{Recall that }\sum_{i=1}^n X_i \sim gamma(n,1):\\
&=\frac{d}{dx}F_{\sum X_i}(x\sqrt{n} + n)\\
&=f_{\sum X_i}(x\sqrt{n} + n)\sqrt{n}\\
&=\frac{1}{\Gamma(n)}(x\sqrt{n}+n)^{n-1}e^{-(x\sqrt{n} + n)}\sqrt{n}\\
&\approx \frac{1}{\sqrt{2\pi}}e^{-x^2/2}\text{ as }n\rightarrow\infty ,
\end{aligned}
\]
  
because of the approximation determined in part (a) and substituting the derivative of the right hand side. Now, for $x=0$ we see that
\[
n! \approx \sqrt{2\pi}n^{n+(1/2)}e^{-n}
\]
  
## Problem 5.39
This exercise, and the two following, will look at some of the mathematical details of convergence.
  
(a) Prove Theorem 5.5.4. (*Hint*: since $h$ is continuous, given $\epsilon > 0$ we can find a $\delta$ such that $|h(x_n) - h(x)| < \epsilon$ whenever $|x_n - x|<\delta$. Translate this into probability statements.)
  
---
  
Theorem 5.5.4 states that if $X_1,X_2,\dots$ converges in probability to a random variable $X$, then if $h$ is a continuous function, $h(X_1),h(X_2),\dots$ converges in probability to $h(X)$.
  
Let's first suppose that $X_1,X_2,\dots$ converge in probability to a random variable $X$. This means that
\[
lim_{n\rightarrow \infty}P(|X_n - X| < \delta) = 1.
\]
  
Consider a continuous function $h$. We want to show that
\[
lim_{n\rightarrow \infty}P(|h(x_n) - h(x)| < \epsilon) = 1.
\]
  
Since $h$ is continuous, we know by theorem that given $\epsilon > 0$ we can find $\delta$ such that $|h(x_n) - h(x)| < \epsilon$ whenever $|x_n - x| < \delta$. That is, $P(|h(x_n) - h(x)| < \epsilon) = 1$ given we have that $|x_n - x| < \delta$, so $P(|h(x_n) - h(x)| < \epsilon)$ comes down knowing under what conditions $|x_n - x| < \delta$. But we've already established that $|x_n - x| < \delta$ is always true as $n \rightarrow \infty$, which implies that $lim_{n\rightarrow \infty}P(|h(x_n) - h(x)| < \epsilon) = 1$ under the starting assumptions.
  
(b) In Example 5.5.8, find a subsequence of the $X_is$ that converges almost surely, that is, converges pointwise.
  
---
  
A sequence of random variables $X_1,X_2,\dots$ *converges almost surely* to a random variable $X$ is, for every $\epsilon > 0$,
\[
P(lim_{n \rightarrow \infty}|X_n - X|<\epsilon) = 1.
\]
  
In Example 5.5.8, the sample space $S$ is the closed interval [0,1], and $X_1,\dots,X_6$ are defined as
\[
\begin{aligned}
&X_1(s) = s + I_{[0,1]}(s)& X_2(s) = s + I_{[0,\frac{1}{2}]}(s) && X_3(s) = s + I_{[\frac{1}{2},1]}(s)\\
&X_4(s) = s + I_{[0,\frac{1}{3}]}(s)& X_5(s) = s + I_{[\frac{1}{3},\frac{2}{3}]}(s) && X_6(s) = s + I_{[\frac{2}{3},1]}(s)
\end{aligned}
\]
  
As the example shows, the sequence of all $X_is$ converges in probability but not pointwise. This will always be the case, as the example shows, depending on the value of $s$. So let's define our subsequence using $s$:
\[
X_i(s) \rightarrow
\begin{cases}
s & \text{if }s>0\\
s+1 & \text{if }s=0\\
\end{cases}
\]
  
## Problem 5.41
Prove Theorem 5.5.13; that is, show that
\[
P(|X_n - \mu| > \epsilon) \rightarrow 0 \text{ for every }\epsilon \iff P(X_n \le x) \rightarrow
\begin{cases}
0 &\text{if }x<\mu\\
1 &\text{if }x \ge \mu.
\end{cases}
\]
  
(a) Set $\epsilon = |x-\mu|$ and show that if $x > \mu$, then $P(X_n \le x) \ge P(|X_n - \mu| \le \epsilon)$, while if $x < \mu$, then $P(X_n \le x) \le P(|X_n-\mu| > \epsilon)$. Deduce the $\implies$ implication.
  
---
  
We will start by showing that if $P(|X_n - \mu| \le \epsilon)\rightarrow 0$, then $P(X_n \le x) \rightarrow 1$ for $x \ge \mu$ and $P(X_n \le x) \rightarrow 0$ for $x < \mu$. Let $\epsilon = |x-\mu|$.
  
Consider the case where $x > \mu$.
\[
\begin{aligned}
P(|X_n - \mu| \le \epsilon) &= P(|X_n - \mu| \le |x-\mu|)\\
&= P(|X_n - \mu| \le x-\mu)\text{ since }x>\mu\\
&= P(-(x-\mu) \le X_n - \mu \le x-\mu)\\
&= P(X_n - \mu \le x - \mu) - P(X_n - \mu \le -(x-\mu))\\
&\le P(X_n - \mu \le x - \mu)\\
&= P(X_n \le x)
\end{aligned}
\]
  
which gives us that
\[
P(X_n \le x) \ge P(|X_n - \mu| \le \epsilon)
\]
  
We know that $P(|X_n - \mu| > \epsilon) \rightarrow 0$ as $n \rightarrow \infty$, implying that $P(|X_n - \mu| \le \epsilon) \rightarrow 1$ as $n \rightarrow \infty$. Therefore, $P(X_n \le x) \rightarrow 1$ for $x > \mu$.
  
Consider the case where $x < \mu$.
\[
\begin{aligned}
P(|X_n - \mu| > \epsilon) &= P(|X_n - \mu| > |x-\mu|)\\
&= P(|X_n - \mu| > -(x-\mu))\\
&=P(X_n - \mu > -(x-\mu)) + P(X_n - \mu < x-\mu)\\
&\ge P(X_n - \mu < x-\mu)\\
&= P(X_n < x)
\end{aligned}
\]
  
such that
\[
P(X_n \le x) \le P(|X_n - \mu|>\epsilon)
\]
  
We began with the assumption that $P(|X_n - \mu| > \epsilon) \rightarrow 0$ when $n \rightarrow \infty$, and since $P(X_n \le x)$ cannot be less than 0, $P(X_n \le x) \rightarrow 0$ as $n \rightarrow \infty$.
  
(b) Use the fact that $\{x:|x-\mu| > \epsilon\} = \{x:x-\mu < -\epsilon\}\cup\{x:x-\mu > \epsilon\}$ to deduce the $\impliedby$ implication.
  
(See Billingsley 1995, Section 25, for a detailed treatment of the above results.)
  
---
  
Now we need to show that if $P(X_n \le x) \rightarrow 0$ when $x<\mu$ and $P(X_n \le x) \rightarrow 1$ when $x > \mu$ then $P(|X_n - \mu| > \epsilon) \rightarrow 0$.
  
Consider $\{x:|x-\mu| > \epsilon\}$






























