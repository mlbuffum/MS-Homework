---
title: "STAT 665 - HW 1"
author: "Maggie Buffum"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem 5.1
Let $x = (X_1,\dots,X_k) \sim N_k(\mu,\Sigma )$, with $r(\Sigma) = k$.
  
(a) Show that
\[
\begin{aligned}
I &= \int_{-\infty}^{\infty} \dots \int_{-\infty}^{\infty}exp\bigg\{-\frac{1}{2}(x - \mu)'\Sigma^{-1}(x  -\mu)\bigg\}dx_1\dots dx_k\\
&=(2\pi)^{k/2}|\Sigma|^{1/2}.
\end{aligned}
\]
  
***
  
What we have is part of a multivariate normal distribution integrated over all possible values of $\mathbf{x}$. Let's first note that by definition of positive-definite, $\Sigma^{-1}$ must be a positive definite matrix, otherwise its inverse would not exist. Now, we know that
\[
f(\mathbf{x};\mu,\Sigma) = \frac{1}{(2\pi)^{k/2}|\Sigma|^{1/2}}exp\bigg\{-\frac{1}{2}(\mathbf{x} - \mu)'\Sigma^{-1}(\mathbf{x} - \mu)\bigg\}\text{ for all }\mathbf{x} \in \mathcal{R}^k
\]
  
and the integration of $f(\mathbf{x};\mu,\Sigma)$ over all possible values of $\mathbf{x}$ is 1. Therefore, we have that
\[
\begin{aligned}
1 &= \int_{-\infty}^{\infty} \dots \int_{-\infty}^{\infty}\frac{1}{(2\pi)^{k/2}|\Sigma|^{1/2}}exp\bigg\{-\frac{1}{2}(x - \mu)'\Sigma^{-1}(x  -\mu)\bigg\}dx_1\dots dx_k\\
&\implies (2\pi)^{k/2}|\Sigma|^{1/2} =  \int_{-\infty}^{\infty} \dots \int_{-\infty}^{\infty}exp\bigg\{-\frac{1}{2}(x - \mu)'\Sigma^{-1}(x  -\mu)\bigg\}dx_1\dots dx_k
\end{aligned}
\]
  
(b) Evaluate $\int_{-\infty}^{\infty}\dots \int_{-\infty}^{\infty} exp\{-(x^2_1 + 2x_1x_2 + 4x_2^2)\}dx_1dx_2$.
  
***
  
Let's rewrite the expression in the following way:
\[
\begin{aligned}
&\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} exp\{-(x^2_1 + 2x_1x_2 + 4x_2^2)\}dx_1dx_2\\
&=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} exp\{-\frac{1}{2}(2x^2_1 + 4x_1x_2 + 8x_2^2)\}dx_1dx_2\\
&=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} exp\{-\frac{1}{2}(\mathbf{x}'\mathbf{A}\mathbf{x})\}dx_1dx_2\\
\end{aligned}
\]
  
where $\mathbf{A} = \begin{bmatrix}2&2\\2&4\end{bmatrix}$, a $2\times 2$ positive-definite matrix. Applying Aitken's integral, we have the solution
\[
\begin{aligned}
&\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} exp\{-\frac{1}{2}(\mathbf{x}'\mathbf{A}\mathbf{x})\}dx_1dx_2\\
&=(2\pi)^{2/2}|\mathbf{A}|^{-1/2}\\
&=(2\pi)\bigg|\begin{bmatrix}2 & 2\\2 & 4\end{bmatrix}\bigg|^{-1/2}\\
&=(2\pi)(4)^{-1/2}\\
&=\pi
\end{aligned}
\]
    
## Problem 5.2
[Graybill, 1961]. Let $x = (X_1,X_2)$ have a bivariate normal distribution with pdf
\[
f(x;\mu,\Sigma) = \frac{1}{k}exp[-Q/2]
\]
  
where $Q = 2x_1^2 - x_1x_2 + 4x_2^2 - 11x_1 - 5x_2 + 19$, and *k* is a constant. Find a constant *a* such that $P(3X_1 - X_2 < a) = 0.01$.
  
***
  
We need to find the pdf of a new random variable $Y_1 = 3X_1 - X_2$. Let $Y_2 = X_1$. Solving for $X_1$ and $X_2$ in terms of $Y_1$ and $Y_2$, we have
\[
\begin{aligned}
X_1 &= Y_2\\
X_2 &= 3Y_2 - Y_1
\end{aligned}
\]
  
Next we solve for the Jacobian:
\[
\mathbf{J} =
\begin{bmatrix}
0 & 1\\
-1 & 3
\end{bmatrix}
= (0)(3) - (1)(-1) = 1
\]
  
Now we can solve for the joint distribution of $Y_1$ and $Y_2$
\[
\begin{aligned}
f(\mathbf{y}) &= f(\mathbf{x})J(\mathbf{y})\\
&=\frac{1}{k}exp\{-Q/2\}(1)\\
&=\frac{1}{k}exp\{-\frac{1}{2}(2x_1^2 - x_1x_2+4x_2^2-11x_1-5x_2 + 19)\}\\
&=\frac{1}{k}exp\{-\frac{1}{2}(\mathbf{x}'\mathbf{B}\mathbf{x}+\mathbf{x}'\mathbf{b} + b_0)\}\\
\end{aligned}
\]
  
Let's solve for $\mathbf{B}$ first:
\[
\begin{aligned}
\mathbf{x}'\mathbf{B}\mathbf{x} &=
\begin{bmatrix}
x_1 & x_2
\end{bmatrix}
\begin{bmatrix}
b_{11} & b_{12}\\
b_{21} & b_{22}
\end{bmatrix}
\begin{bmatrix}
x_1\\x_2
\end{bmatrix}\\
&=\begin{bmatrix}
b_{11}x_1 + b_{21}x_2 & b_{12}x_1 + b_{22}x_2
\end{bmatrix}
\begin{bmatrix}
x_1\\x_2
\end{bmatrix}\\
&=b_{11}x_1^2 + b_{21}x_1x_2 + b_{12}x_1x_2 + b_{22}x_2^2\\
&=(2)x_1^2 + (-1/2)x_1x_2 + (-1/2)x_1x_2 + (4)x_2^2\\
&\implies\mathbf{B} =
\begin{bmatrix}
2 & -1/2\\
-1/2 & 4
\end{bmatrix}
\end{aligned}
\]
  
and now $\mathbf{b}$:
\[
\begin{aligned}
\mathbf{x}'\mathbf{b} &=\begin{bmatrix}
x_1 & x_2
\end{bmatrix}
\begin{bmatrix}
b_1\\b_2
\end{bmatrix}\\
&=b_1x_1 + b_2x_2\\
&\implies \mathbf{b} =
\begin{bmatrix}
-11\\
-5
\end{bmatrix}
\end{aligned}
\]
  
and clearly $b_0 = 19$.
  
## Problem 5.5
(a) Show that $(X_1,X_2)$ has a bivariate normal distribution with means $\mu_1,\mu_2$, variances $\sigma_1^2,\sigma_2^2$, and a correlation coefficient $\rho$ if and only if every linear combination $c_1X_1 + c_2X_2$ has a univariate normal distribution with mean $c_1\mu_1 + c_2\mu_2$, and variance $c_1^2\sigma_1^2 + c_2^2\sigma_2^2 + 2c_1c_2\rho\sigma_{1,2}$, where $c_1$ and $c_2$ are real constants, not both equal to zero.
  
***
  
Let's first show that if $(X_1,X_2)$ has a bivariate normal distribution with means $\mu_1,\mu_2$, variances $\sigma_1^2,\sigma_2^2$, and a correlation coefficient $\rho$ then every linear combination $c_1X_1 + c_2X_2$ has a univariate normal distribution with mean $c_1\mu_1 + c_2\mu_2$ and variance $c_1^2\sigma_1^2 + c_2\sigma_2^2 + 2c_1c_2\rho\sigma_{1,2}$.
  
Let $(X_1,X_2)\sim N(\mu_1,\mu_2,\sigma_1^2,\sigma_2^2,\rho)$. Consider the transformation $Y_1 = c_1X_1 + c_2X_2$. Let $Y_2 = X_2$. We can solve for $X_1$ and $X_2$:
\[
\begin{aligned}
X_2 &= Y_2\\
X_1 &= \frac{1}{c_1}(Y_1 - c_2Y_2)
\end{aligned}
\]
  
The Jacobian is
\[
\mathbf{J} =
\begin{bmatrix}
\frac{1}{c_1} & -\frac{1}{c_2}\\
0 & 1
\end{bmatrix}
= \frac{1}{c_1}
\]
  
(b) Let $Y_i = X_i/\sigma_i,i=1,2$. Show that $Var(Y_1 - Y_2)=2(1 - \rho)$.
  
## Problem 5.6
(a) Let $(X_1,X_2) \sim N_2(\mu_1,\mu_2,\sigma_1^2,\sigma_2^2,\rho)$ where $\mu_1=\mu_2=0$ and $\rho \ne 1$. The polar coordinate transformation is defined by $X_1 = Rcos\Theta$, $X_2 = Rsin\Theta$. Show that the joint pdf of $R$ and $\Theta$ is given by
\[
r(2\pi)^{-1}(1-\rho^2)^{-1/2}exp\bigg[-\frac{1}{2(1-\rho^2)}r^2(1 - \rho sin 2\theta)\bigg],
\]
  
$0 \le r < \infty$, and $0 \le \theta \le 2\pi$, and that the marginal pdf of $\Theta$ is
\[
(2\pi)^{-1}(1-\rho^2)^{1/2}(1-\rho sin 2\theta)^{-1},\ 0 \le \theta \le 2\pi.
\]
  
(b) Suppose $(X_1,X_2)$ has a bivariate normal distribution $N_2(0,0,\sigma_1^2,\sigma_2^2,\rho$, $|\rho| \ne 1$. Show that
\[
P(X_1 > 0,X_2 > 0) = \frac{1}{4} + \frac{1}{2\pi}sin^{-1}(\rho).
\]
  
## Problem 5.7
The random vector $x = (X_1,X_2,\dots,X_k)'$ is said to have a symmetric multivariate normal distribution if $x \sim N_k(\mu,\Sigma)$ where $\mu = \mu1_k$, i.e., the mean of each $X_j$ is equal to the same constant $\mu$, and $\Sigma$ is the equicorrelation dispersion matrix, i.e.,
\[
\Sigma = \sigma^2 =
\begin{bmatrix}
1 & \rho & \dots & \rho\\
\rho & 1 & \dots & \rho\\
\vdots & \vdots & \ddots & \vdots\\
\rho & \rho & \dots & 1
\end{bmatrix}
\]
  
When $k=3$, $\mu=0$, $\sigma^2 = 2$, and $\rho = 1/2$, find the probability that $X_3 = min(X_1,X_2.X_3)$.
  
(*Hint:* Recall that if $x = (X_1,\dots,X_k)'$ has a continuous symmetric distribution, then all possible permutations of $X_1,\dots,X_k$ are equally likely, each having probability $P(X_{i1} < \dots < X_{ik})=1/k!$ for any permutation $(i1,\dots,i_k)$ for the first *k* positive integers.
  
## Problem 5.8
Let $x \sim N_k(0,\Sigma)$ with pdf $f(x)$ where $\Sigma = \{\Sigma_{ij}\}$. The entropy $h(x)$ is defined as
\[
h(x) = - \int f(x) lnf(x)
\]
  
(a) Show that $h(x) = \frac{1}{2}ln(2\pi e)^k |\Sigma|$.
  
(b) Hence, or otherwise, show that $|\Sigma| \le \prod_{i=1}^k \Sigma_{ii}$, with equality holding if and only if $\Sigma_{ij} = 0$, for $i \ne j$ [Hadamard's inequality].

































