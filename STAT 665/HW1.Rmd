---
title: "STAT 665 - HW 1"
author: "Maggie Buffum"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem 5.1
Let $x = (X_1,\dots,X_k) \sim N_k(\mu,\Sigma )$, with $r(\Sigma) = k$.
  
(a) Show that
\[
\begin{aligned}
I &= \int_{-\infty}^{\infty} \dots \int_{-\infty}^{\infty}exp\bigg\{-\frac{1}{2}(x - \mu)'\Sigma^{-1}(x  -\mu)\bigg\}dx_1\dots dx_k\\
&=(2\pi)^{k/2}|\Sigma|^{1/2}.
\end{aligned}
\]
  
***
  
What we have is part of a multivariate normal distribution integrated over all possible values of $\mathbf{x}$. Let's first note that by definition of positive-definite, $\Sigma^{-1}$ must be a positive definite matrix, otherwise its inverse would not exist. Now, we know that
\[
f(\mathbf{x};\mu,\Sigma) = \frac{1}{(2\pi)^{k/2}|\Sigma|^{1/2}}exp\bigg\{-\frac{1}{2}(\mathbf{x} - \mu)'\Sigma^{-1}(\mathbf{x} - \mu)\bigg\}\text{ for all }\mathbf{x} \in \mathcal{R}^k
\]
  
and the integration of $f(\mathbf{x};\mu,\Sigma)$ over all possible values of $\mathbf{x}$ is 1. Therefore, we have that
\[
\begin{aligned}
1 &= \int_{-\infty}^{\infty} \dots \int_{-\infty}^{\infty}\frac{1}{(2\pi)^{k/2}|\Sigma|^{1/2}}exp\bigg\{-\frac{1}{2}(x - \mu)'\Sigma^{-1}(x  -\mu)\bigg\}dx_1\dots dx_k\\
&\implies (2\pi)^{k/2}|\Sigma|^{1/2} =  \int_{-\infty}^{\infty} \dots \int_{-\infty}^{\infty}exp\bigg\{-\frac{1}{2}(x - \mu)'\Sigma^{-1}(x  -\mu)\bigg\}dx_1\dots dx_k
\end{aligned}
\]
  
(b) Evaluate $\int_{-\infty}^{\infty}\dots \int_{-\infty}^{\infty} exp\{-(x^2_1 + 2x_1x_2 + 4x_2^2)\}dx_1dx_2$.
  
***
  
Let's rewrite the expression in the following way:
\[
\begin{aligned}
&\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} exp\{-(x^2_1 + 2x_1x_2 + 4x_2^2)\}dx_1dx_2\\
&=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} exp\{-\frac{1}{2}(2x^2_1 + 4x_1x_2 + 8x_2^2)\}dx_1dx_2\\
&=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} exp\{-\frac{1}{2}(\mathbf{x}'\mathbf{A}\mathbf{x})\}dx_1dx_2\\
\end{aligned}
\]
  
where $\mathbf{A} = \begin{bmatrix}2&2\\2&8\end{bmatrix}$, a $2\times 2$ positive-definite matrix. Applying Aitken's integral, we have the solution
\[
\begin{aligned}
&\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} exp\{-\frac{1}{2}(\mathbf{x}'\mathbf{A}\mathbf{x})\}dx_1dx_2\\
&=(2\pi)^{2/2}|\mathbf{A}|^{-1/2}\\
&=(2\pi)\bigg|\begin{bmatrix}2 & 2\\2 & 8\end{bmatrix}\bigg|^{-1/2}\\
&=(2\pi)((2)(8) - (2)(2))^{-1/2}\\
&=(2\pi)(12)^{-1/2}\\
&=(2\pi)\frac{2}{\sqrt{3}}\\
&=\frac{\pi}{\sqrt{3}}
\end{aligned}
\]
    
## Problem 5.2
[Graybill, 1961]. Let $x = (X_1,X_2)$ have a bivariate normal distribution with pdf
\[
f(x;\mu,\Sigma) = \frac{1}{k}exp[-Q/2]
\]
  
where $Q = 2x_1^2 - x_1x_2 + 4x_2^2 - 11x_1 - 5x_2 + 19$, and *k* is a constant. Find a constant *a* such that $P(3X_1 - X_2 < a) = 0.01$.
  
---
  
Since we know the distribution of the vector $\mathbf{x}$ is normal, $Q = (\mathbf{x} - \mathbf{\mu})'\Sigma^{-1}(\mathbf{x} - \mathbf{\mu})$. We can solve for $\Sigma$ as
\[
\begin{aligned}
Q =&
\begin{bmatrix}
x_1 - \mu_1&x_2-\mu_2
\end{bmatrix}
\begin{bmatrix}
\sigma_1 & \sigma_{12}\\
\sigma_{12} & \sigma_2
\end{bmatrix}^{-1}
\begin{bmatrix}
x_1 - \mu_1&x_2-\mu_2
\end{bmatrix}\\
=& \begin{bmatrix}
x_1 - \mu_1&x_2-\mu_2
\end{bmatrix}
\frac{1}{|\Sigma|}
\begin{bmatrix}
\sigma_2 & -\sigma_{12}\\
-\sigma_{12} & \sigma_1
\end{bmatrix}
\begin{bmatrix}
x_1 - \mu_1&x_2-\mu_2
\end{bmatrix}\\
=& \begin{bmatrix}
x_1 - \mu_1&x_2-\mu_2
\end{bmatrix}
\begin{bmatrix}
\frac{\sigma_2}{|\Sigma|} & -\frac{\sigma_{12}}{|\Sigma|}\\
-\frac{\sigma_{12}}{|\Sigma|} & \frac{\sigma_1}{|\Sigma|}
\end{bmatrix}
\begin{bmatrix}
x_1 - \mu_1&x_2-\mu_2
\end{bmatrix}\\
=&
\begin{bmatrix}
(x_1-\mu_1)\frac{\sigma_2}{|\Sigma|} - (x_2 - \mu_2)\frac{\sigma_{12}}{|\Sigma|} & -(x_1 - \mu_1)\frac{\sigma_{12}}{|\Sigma|} + (x_2 - \mu_2)\frac{\sigma_1}{|\Sigma|}
\end{bmatrix}
\begin{bmatrix}
x_1 - \mu_1&x_2-\mu_2
\end{bmatrix}\\
=&
(x_1-\mu_1)^2\frac{\sigma_2}{|\Sigma|} - (x_2 - \mu_2)\frac{\sigma_{12}}{|\Sigma|}(x_1-\mu_1) - (x_1 - \mu_1)\frac{\sigma_{12}}{|\Sigma|}(x_2-\mu_2) + (x_2 - \mu_2)^2\frac{\sigma_1}{|\Sigma|}\\
=&\frac{\sigma_2}{|\Sigma|}(x_1-\mu_2)^2 + \frac{\sigma_{1}}{|\Sigma|}(x_2-\mu_2)^2 - 2\frac{\sigma_{12}}{|\Sigma|}(x_1 - \mu_1)(x_2 - \mu_2)\\
=& \frac{\sigma_2}{|\Sigma|}(x_1^2 + \mu_1^2 - 2x_1\mu_1) + \frac{\sigma_{1}}{|\Sigma|}(x_2^2 + \mu_2^2 - 2x_2\mu_2) - 2\frac{\sigma_{12}}{|\Sigma|}(x_1x_2 - x_1\mu_2 - x_2\mu_1 + \mu_1\mu_2)\\
=&\frac{\sigma_2}{|\Sigma|}x_1^2 + \frac{\sigma_{1}}{|\Sigma|}x_2^2 - 2\frac{\sigma_{12}}{|\Sigma|}x_1x_2 + \frac{\sigma_2}{|\Sigma|}\mu_1^2 - 2\frac{\sigma_2}{|\Sigma|}x_1\mu_1 + \frac{\sigma_{1}}{|\Sigma|}\mu_2^2 - 2\frac{\sigma_{1}}{|\Sigma|}x_2\mu_2 + 2\frac{\sigma_{12}}{|\Sigma|}x_1\mu_2 + 2\frac{\sigma_{12}}{|\Sigma|}x_2\mu_1 - 2\frac{\sigma_{12}}{|\Sigma|}\mu_1\mu_2\\
=&\frac{\sigma_2}{|\Sigma|}x_1^2 + \frac{\sigma_{1}}{|\Sigma|}x_2^2 - 2\frac{\sigma_{12}}{|\Sigma|}x_1x_2 +
x_1\bigg(- 2\frac{\sigma_2}{|\Sigma|}\mu_1+ 2\frac{\sigma_{12}}{|\Sigma|}\mu_2\bigg) + x_2\bigg(- 2\frac{\sigma_{1}}{|\Sigma|}\mu_2+ 2\frac{\sigma_{12}}{|\Sigma|}\mu_1\bigg)  + \frac{\sigma_2}{|\Sigma|}\mu_1^2  + \frac{\sigma_{1}}{|\Sigma|}\mu_2^2 - 2\frac{\sigma_{12}}{|\Sigma|}\mu_1\mu_2\\
=&(2)x_1^2 + (4)x_2^2 - 2(1/2)x_1x_2 +
x_1\bigg(- 2(2)\mu_1+ 2(1/2)\mu_2\bigg) + x_2\bigg(- 2(4)\mu_2+ 2(1/2)\mu_1\bigg)  + (2)\mu_1^2  + (4)\mu_2^2 - 2(1/2)\mu_1\mu_2\\
\end{aligned}
\]
  
Now we know that
\[
\begin{aligned}
-11 &= - 2(2)\mu_1+ 2(1/2)\mu_2\text{ and}\\
-5 &= - 2(4)\mu_2+ 2(1/2)\mu_1\\
\end{aligned}
\]
  
giving us that $\mu_1 = 3$ and $\mu_2=1$. Note that
\[
\begin{aligned}
&(2)\mu_1^2  + (4)\mu_2^2 - 2(1/2)\mu_1\mu_2\\
&=(2)(3)^2  + (4)(1)^2 - (3)(1)\\
&=18  + 4 - 3\\
&=19
\end{aligned}
\]
  
We now know what $\Sigma$ is too:
\[
\begin{aligned}
\Sigma &= \frac{4}{31}
\begin{bmatrix}
4 & -1/2\\
-1/2 & 2
\end{bmatrix}\\
\end{aligned}
\]
  
Now we can look at the random variable $Y = 3X_1 - X_2$, which has expected value $EY = 3EX_1 - EX_2 = 3(3) - 1 = 8$. Its variance is
\[
\begin{aligned}
Var(Y) &= Var(3X_1 - X_2)\\
&=9Var(X_1) + Var(X_2) - 2(3)Cov(X_1,X_2)\\
&=9\frac{16^2}{31^2} + \frac{8^2}{31^2} - 6\frac{2^2}{31^2}\\
&=2.439126
\end{aligned}
\]
  
## Problem 5.5
(a) Show that $(X_1,X_2)$ has a bivariate normal distribution with means $\mu_1,\mu_2$, variances $\sigma_1^2,\sigma_2^2$, and a correlation coefficient $\rho$ if and only if every linear combination $c_1X_1 + c_2X_2$ has a univariate normal distribution with mean $c_1\mu_1 + c_2\mu_2$, and variance $c_1^2\sigma_1^2 + c_2^2\sigma_2^2 + 2c_1c_2\rho\sigma_{1,2}$, where $c_1$ and $c_2$ are real constants, not both equal to zero.
  
***
  
Let's first show that if $(X_1,X_2)$ has a bivariate normal distribution with means $\mu_1,\mu_2$, variances $\sigma_1^2,\sigma_2^2$, and a correlation coefficient $\rho$ then every linear combination $c_1X_1 + c_2X_2$ has a univariate normal distribution with mean $c_1\mu_1 + c_2\mu_2$ and variance $c_1^2\sigma_1^2 + c_2\sigma_2^2 + 2c_1c_2\rho\sigma_{1,2}$.
  
Let $(X_1,X_2)\sim N(\mu_1,\mu_2,\sigma_1^2,\sigma_2^2,\rho)$. Consider the transformation $Y_1 = c_1X_1 + c_2X_2$. Let $Y_2 = X_2$. We can solve for $X_1$ and $X_2$:
\[
\begin{aligned}
X_2 &= Y_2\\
X_1 &= \frac{1}{c_1}(Y_1 - c_2Y_2)
\end{aligned}
\]
  
The Jacobian is
\[
\mathbf{J} =
\begin{bmatrix}
\frac{1}{c_1} & -\frac{1}{c_2}\\
0 & 1
\end{bmatrix}
= \frac{1}{c_1}
\]
  
(b) Let $Y_i = X_i/\sigma_i,i=1,2$. Show that $Var(Y_1 - Y_2)=2(1 - \rho)$.
  
## Problem 5.6
(a) Let $(X_1,X_2) \sim N_2(\mu_1,\mu_2,\sigma_1^2,\sigma_2^2,\rho)$ where $\mu_1=\mu_2=0$ and $\rho \ne 1$. The polar coordinate transformation is defined by $X_1 = Rcos\Theta$, $X_2 = Rsin\Theta$. Show that the joint pdf of $R$ and $\Theta$ is given by
\[
r(2\pi)^{-1}(1-\rho^2)^{-1/2}exp\bigg[-\frac{1}{2(1-\rho^2)}r^2(1 - \rho sin 2\theta)\bigg],
\]
  
$0 \le r < \infty$, and $0 \le \theta \le 2\pi$, and that the marginal pdf of $\Theta$ is
\[
(2\pi)^{-1}(1-\rho^2)^{1/2}(1-\rho sin 2\theta)^{-1},\ 0 \le \theta \le 2\pi.
\]
  
---
  
To complete the transformation, we need to find the Jacobian:
\[
\begin{aligned}
J &=
\begin{bmatrix}
\cos\theta &-r\sin\theta\\
\sin\theta & r\cos\theta
\end{bmatrix}
=\cos\theta r\cos\theta - (-r\sin\theta \sin\theta)\\
&=r\cos^2\theta + r\sin^2\theta\\
&=r(\cos^2\theta + \sin^2\theta)\\
&=r
\end{aligned}
\]
  
The bivariate normal distribution is
\[
\begin{aligned}
f(x_1,x_2) &= \frac{1}{2\pi|\Sigma|^{1/2}}\exp\bigg\{-\frac{1}{2}\frac{1}{1-\rho^2}\bigg(\frac{x_1^2}{\sigma_1^2}+\frac{x_2^2}{\sigma_2^2} - 2\rho\frac{x_1x_2}{\sigma_1\sigma_2}\bigg)\bigg\}\\
&=\frac{1}{2\pi\sqrt{\sigma_1^2\sigma_2^2(1-\rho^2)}}\exp\bigg\{-\frac{1}{2}\frac{1}{1-\rho^2}\bigg(\frac{x_1^2}{\sigma_1^2}+\frac{x_2^2}{\sigma_2^2} - 2\rho\frac{x_1x_2}{\sigma_1\sigma_2}\bigg)\bigg\}
\end{aligned}
\]
  
Now we can find the distribution of the transformed variables:
\[
\begin{aligned}
f(r,\theta) &= \frac{1}{2\pi\sqrt{\sigma_1^2\sigma_2^2(1-\rho^2)}}\exp\bigg\{-\frac{1}{2}\frac{1}{1-\rho^2}\bigg(\frac{x_1^2}{\sigma_1^2}+\frac{x_2^2}{\sigma_2^2} - 2\rho\frac{x_1x_2}{\sigma_1\sigma_2}\bigg)\bigg\}|r|\\
&= \frac{1}{2\pi\sqrt{\sigma_1^2\sigma_2^2(1-\rho^2)}}\exp\bigg\{-\frac{1}{2}\frac{1}{1-\rho^2}\bigg(\frac{r^2\cos^2\theta}{\sigma_1^2}+\frac{r^2\sin^2\theta}{\sigma_2^2} - 2\rho\frac{r\cos\theta r\sin\theta}{\sigma_1\sigma_2}\bigg)\bigg\}|r|\\
&= \frac{|r|}{2\pi\sqrt{\sigma_1^2\sigma_2^2(1-\rho^2)}}\exp\bigg\{-\frac{1}{2}\frac{r^2}{1-\rho^2}\bigg(\frac{\cos^2\theta\sigma_2^2}{\sigma_1^2\sigma_2^2}+\frac{\sin^2\theta\sigma_1^2}{\sigma_1^2\sigma_2^2} - 2\rho\frac{\cos\theta \sin\theta}{\sigma_1\sigma_2}\bigg)\bigg\}\\
&= \frac{|r|}{2\pi\sqrt{\sigma_1^2\sigma_2^2(1-\rho^2)}}\exp\bigg\{-\frac{1}{2}\frac{r^2}{1-\rho^2}\bigg(\frac{\cos^2\theta\sigma_2^2 + \sin^2\theta\sigma_1^2 - \rho \sin2\theta}{\sigma_1\sigma_2}\bigg)\bigg\}\\
\end{aligned}
\]
  
The only way for this to work is if we assume $\sigma_1=\sigma_2=1$. Then we have shown equality:
\[
\begin{aligned}
&= \frac{|r|}{2\pi\sqrt{(1-\rho^2)}}\exp\bigg\{-\frac{1}{2}\frac{r^2}{1-\rho^2}\bigg(\cos^2\theta\ + \sin^2\theta - \rho \sin2\theta\bigg)\bigg\}\\
&= \frac{|r|}{2\pi\sqrt{(1-\rho^2)}}\exp\bigg\{-\frac{1}{2}\frac{r^2}{1-\rho^2}\bigg(1 - \rho \sin2\theta\bigg)\bigg\},\text{ for }r \ge 0,0 \le \theta \le 2\pi\\
\end{aligned}
\]
  
(b) Suppose $(X_1,X_2)$ has a bivariate normal distribution $N_2(0,0,\sigma_1^2,\sigma_2^2,\rho$, $|\rho| \ne 1$. Show that
\[
P(X_1 > 0,X_2 > 0) = \frac{1}{4} + \frac{1}{2\pi}sin^{-1}(\rho).
\]
  
## Problem 5.7
The random vector $x = (X_1,X_2,\dots,X_k)'$ is said to have a symmetric multivariate normal distribution if $x \sim N_k(\mu,\Sigma)$ where $\mu = \mu1_k$, i.e., the mean of each $X_j$ is equal to the same constant $\mu$, and $\Sigma$ is the equicorrelation dispersion matrix, i.e.,
\[
\Sigma = \sigma^2 =
\begin{bmatrix}
1 & \rho & \dots & \rho\\
\rho & 1 & \dots & \rho\\
\vdots & \vdots & \ddots & \vdots\\
\rho & \rho & \dots & 1
\end{bmatrix}
\]
  
When $k=3$, $\mu=0$, $\sigma^2 = 2$, and $\rho = 1/2$, find the probability that $X_3 = min(X_1,X_2.X_3)$.
  
(*Hint:* Recall that if $x = (X_1,\dots,X_k)'$ has a continuous symmetric distribution, then all possible permutations of $X_1,\dots,X_k$ are equally likely, each having probability $P(X_{i1} < \dots < X_{ik})=1/k!$ for any permutation $(i1,\dots,i_k)$ for the first *k* positive integers.
  
## Problem 5.8
Let $\mathbf{x} \sim N_k(0,\Sigma)$ with pdf $f(x)$ where $\Sigma = \{\Sigma_{ij}\}$. The entropy $h(x)$ is defined as
\[
h(x) = - \int f(x) lnf(x)
\]
  
(a) Show that $h(x) = \frac{1}{2}ln\big[(2\pi e)^k |\Sigma|\big]$.
  
---
  
We need to show that
\[
\frac{1}{2}\ln(2\pi)^k|\Sigma|=-\int f(x)\ln(f(x)) 
\]
  
But we know $\mathbf{x}$ has a multivariate normal distribution $N_k(0,\Sigma)$:
\[
f(\mathbf{x}) = \frac{1}{(2\pi)^{k/2}|\Sigma|^{1/2}}\exp\bigg\{-\frac{1}{2}(\mathbf{x} - \mu)'\Sigma^{-1}(\mathbf{x} - \mu)\bigg\},\ \mathbf{x} \in R^k
\]
  
Inserting into the definition of entropy, we have
\[
\begin{aligned}
-\int&\frac{1}{(2\pi)^{k/2}|\Sigma|^{1/2}}\exp\bigg\{-\frac{1}{2}(\mathbf{x} - \mu)'\Sigma^{-1}(\mathbf{x} - \mu)\bigg\}\ln\bigg(\frac{1}{(2\pi)^{k/2}|\Sigma|^{1/2}}\exp\bigg\{-\frac{1}{2}(\mathbf{x} - \mu)'\Sigma^{-1}(\mathbf{x} - \mu)\bigg\}\bigg)\\
&=-\int\frac{1}{(2\pi)^{k/2}|\Sigma|^{1/2}}\exp\bigg\{-\frac{1}{2}(\mathbf{x}'\Sigma^{-1}\mathbf{x} + \mathbf{x}'\mathbf{\mu})\bigg\}\ln\bigg(\frac{1}{(2\pi)^{k/2}|\Sigma|^{1/2}}\exp\bigg\{-\frac{1}{2}(\mathbf{x}'\Sigma^{-1}\mathbf{x} + \mathbf{x}'\mathbf{\mu})\bigg\}\bigg)\\
&=-\int\frac{1}{(2\pi)^{k/2}|\Sigma|^{1/2}}\exp\bigg\{-\frac{1}{2}(\mathbf{x}'\Sigma^{-1}\mathbf{x})\bigg\}\ln\bigg(\frac{1}{(2\pi)^{k/2}|\Sigma|^{1/2}}\exp\bigg\{-\frac{1}{2}(\mathbf{x}'\Sigma^{-1}\mathbf{x})\bigg\}\bigg)\\
\end{aligned}
\]
  
since we were given that $\mathbf{\mu} = \mathbf{0}$. Applying log rules, we can expand the last term and continue:
\[
\begin{aligned}
=&-\int\frac{1}{(2\pi)^{k/2}|\Sigma|^{1/2}}\exp\bigg\{-\frac{1}{2}(\mathbf{x}'\Sigma^{-1}\mathbf{x})\bigg\}\bigg(-\frac{1}{2}(\mathbf{x}'\Sigma^{-1}\mathbf{x}) - \frac{k}{2}\ln(2\pi) - \frac{1}{2}\ln(|\Sigma|)\big)\bigg)\\
=&\int\frac{1}{(2\pi)^{k/2}|\Sigma|^{1/2}}\exp\bigg\{-\frac{1}{2}(\mathbf{x}'\Sigma^{-1}\mathbf{x})\bigg\}\bigg(\frac{1}{2}(\mathbf{x}'\Sigma^{-1}\mathbf{x}) + \frac{k}{2}\ln(2\pi) + \frac{1}{2}\ln(|\Sigma|)\bigg)\\
=&\frac{1}{2}\int\frac{1}{(2\pi)^{k/2}|\Sigma|^{1/2}}\exp\bigg\{-\frac{1}{2}(\mathbf{x}'\Sigma^{-1}\mathbf{x})\bigg\}\bigg((\mathbf{x}'\Sigma^{-1}\mathbf{x}) + k\ln(2\pi) + \ln(|\Sigma|)\bigg)\\
=&\frac{1}{2}\bigg[\frac{1}{(2\pi)^{k/2}|\Sigma|^{1/2}}\int(\mathbf{x}'\Sigma^{-1}\mathbf{x})\exp\bigg\{-\frac{1}{2}(\mathbf{x}'\Sigma^{-1}\mathbf{x})\bigg\}\\
&+ k\ln(2\pi)\int\frac{1}{(2\pi)^{k/2}|\Sigma|^{1/2}}\exp\bigg\{-\frac{1}{2}(\mathbf{x}'\Sigma^{-1}\mathbf{x})\bigg\}\\
&+ \ln(|\Sigma|)\int\frac{1}{(2\pi)^{k/2}|\Sigma|^{1/2}}\exp\bigg\{-\frac{1}{2}(\mathbf{x}'\Sigma^{-1}\mathbf{x})\bigg\}\bigg)\bigg]\\
\end{aligned}
\]
  
The integrals in the last two terms simply equal the constants pulled out front as we are integrating the multinomial normal distribution across all values of $\mathbf{x}$. The first term can be evaluated per Result 5.1.3:
\[
\begin{aligned}
&\frac{1}{2}\bigg[\frac{1}{(2\pi)^{k/2}|\Sigma|^{1/2}}(2\pi)^{k/2}|\Sigma^{-1}|^{-1/2}tr(\Sigma^{-1}\Sigma)+ k\ln(2\pi)+ \ln(|\Sigma|)\bigg]\\
&=\frac{1}{2}\bigg[\frac{1}{|\Sigma|^{1/2}}|\Sigma|^{1/2}tr(I_k) + k\ln(2\pi)+ \ln(|\Sigma|)\bigg]\\
&=\frac{1}{2}\bigg[k + k\ln(2\pi)+ \ln(|\Sigma|)\bigg]\\
&=\frac{1}{2}\bigg[k + \ln\big[(2\pi)^k|\Sigma|\big]\bigg]\\
&=\frac{1}{2}\bigg[\ln(e^k) + \ln\big[(2\pi)^k|\Sigma|\big]\bigg]\\
&=\frac{1}{2}\ln\big[(2\pi e)^k|\Sigma|\big]
\end{aligned}
\]
  
(b) Hence, or otherwise, show that $|\Sigma| \le \prod_{i=1}^k \Sigma_{ii}$, with equality holding if and only if $\Sigma_{ij} = 0$, for $i \ne j$ [Hadamard's inequality].
  
From part (a) we know that $-\int f(x)\ln(f(x)) = \frac{1}{2}\ln\big[(2\pi e)^k|\Sigma|\big]$.
  
Let's look at the determinant of $\Sigma$. Consider that we hold $i$ fixed at one; then
  
\[
\begin{aligned}
& -\int f(x) \ln(f(x)) = \frac{1}{2}\ln((2\pi e)^k|\Sigma|)\\
& \implies -2\int f(x) \ln(f(x)) = \ln((2\pi e)^k|\Sigma|)\\
& \implies -2\int f(x) \ln(f(x)) = \ln((2\pi e)^k) + \ln(|\Sigma|)\\
& \implies -2\int f(x) \ln(f(x)) - \ln((2\pi e)^k)= \ln(|\Sigma|)\\
& \implies \exp\bigg\{-2\int f(x) \ln(f(x)) - \ln((2\pi e)^k)\bigg\}= |\Sigma|\\
\end{aligned}
\]
































